<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>HW 1: Computer Vision in Autonomous Vehicles - The Technology Behind Computer Vision</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Saira:ital,wght@0,100..900;1,100..900&display=swap"
        rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet">
</head>

<body>
    <header>
        <h1>The Technology Behind Computer Vision</h1>
    </header>
    <hr>
    <nav>
        <a href="index.html">Home</a>
        <a href="how_it_works.html" class="active">How It Works</a>
        <a href="applications.html">Applications</a>
        <a href="challenges_and_future.html">Challenges & Future</a>
    </nav>

    <br><br>

    <main>
        <figure>
            <img class="main-img" src="img/av.png" alt="">
            <figcaption></figcaption>
        </figure>

        <div class="sections">

            <section>
                <h2 id="pipeline">Perception Pipeline</h2>
                <ol>
                    <li><strong>Capture:</strong> Cameras/LiDAR/Radar collect raw data.</li>
                    <li><strong>Preprocess:</strong> Undistort images, normalize, timestamp sync.</li>
                    <li><strong>Perception:</strong> Detect, classify, and track objects and lanes.</li>
                    <li><strong>Fusion:</strong> Merge multiple sensors into a single world model.</li>
                    <li><strong>Prediction:</strong> Estimate where objects will be next.</li>
                    <li><strong>Planning:</strong> Choose a safe path and speed.</li>
                </ol>
            </section>

            <section>
                <h2 id="sensors">Sensor Suite</h2>
                <table>
                    <caption>Common Sensors in Autonomous Vehicles</caption>
                    <thead>
                        <tr>
                            <th scope="col">Sensor</th>
                            <th scope="col">Strengths</th>
                            <th scope="col">Limitations</th>
                            <th scope="col">Typical Uses</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <th scope="row">Cameras</th>
                            <td>High detail; color &amp; texture; sign/light recognition</td>
                            <td>Sensitive to lighting, glare, weather</td>
                            <td>Lane detection, traffic lights/signs, object classification</td>
                        </tr>
                        <tr>
                            <th scope="row">LiDAR</th>
                            <td>Accurate depth; 3D structure; robust to lighting</td>
                            <td>Costly; moving parts; reduced range in heavy rain/fog</td>
                            <td>3D mapping, obstacle detection, localization</td>
                        </tr>
                        <tr>
                            <th scope="row">Radar</th>
                            <td>Great range &amp; velocity; works in bad weather</td>
                            <td>Lower resolution; harder to classify</td>
                            <td>Long-range vehicles detection, adaptive cruise</td>
                        </tr>
                        <tr>
                            <th scope="row">Ultrasonic</th>
                            <td>Close-range accuracy; inexpensive</td>
                            <td>Very short range</td>
                            <td>Parking assist, curb &amp; bumper proximity</td>
                        </tr>
                        <tr>
                            <th scope="row">IMU/GNSS</th>
                            <td>Position &amp; motion estimates</td>
                            <td>GPS drift; tunnels &amp; urban canyons</td>
                            <td>Localization, stabilization, map alignment</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section>
                <h2 id="ml">Core Vision Tasks &amp; Models</h2>
                <dl>
                    <dt>Object Detection</dt>
                    <dd>Finds and localizes cars, pedestrians, cyclists (e.g., with bounding boxes).</dd>

                    <dt>Semantic Segmentation</dt>
                    <dd>Labels each pixel (road, sidewalk, sky) to understand drivable space.</dd>

                    <dt>Instance Segmentation</dt>
                    <dd>Separates different instances of the same class (two pedestrians, not one blob).</dd>

                    <dt>Multi-Object Tracking (MOT)</dt>
                    <dd>Assigns identities across frames to estimate trajectories.</dd>

                    <dt>Depth Estimation</dt>
                    <dd>Predicts distance to objects; can be LiDAR- or vision-based (stereo/monocular).</dd>

                    <dt>Lane/Marker Detection</dt>
                    <dd>Identifies lane boundaries and topology for lane keeping and planning.</dd>
                </dl>
                <aside>
                    <p><strong>Tip:</strong> Combining multiple tasks improves robustnessâ€”e.g., detection + segmentation
                        for cleaner drivable areas.</p>
                </aside>
            </section>

        </div>
    </main>



    <footer>
        <hr>
        <img src="img/logo-white.svg" alt="CSUMB logo" height="40px">
        <br>
        CST336 Internet Programming. 2025 &copy; Cheng <br>
        <strong>Disclaimer:</strong> The information in this webpage is fictitious. <br>
    </footer>

</body>

</html>